随着大型网站的各种高并发访问、海量数据处理等场景越来越多，如何实现网站的高可用、易伸缩、可扩展、安全等目标就显得越来越重要。
为了解决这样一系列问题，大型网站的架构也在不断发展。提高大型网站的高可用架构，不得不提的就是分布式。

一、分布式定义：
distributed system
分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。
首先，这种系统起码由好几台主机组成。以谷歌、亚马逊等服务商而言，他们的数据中心都由上万台主机支撑起来的。
其次，对于外人来说是感觉不到这些主机的存在。我们只看到是一个系统在运作。以最近的“亚马逊 S3 宕机事件”为例，平时我们压根不知道亚马逊所提供的服务背后是由多少台主机组成，
但是等到S3宕机才知道它已经是占了互联网世界的半壁江山了。

分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。
从分布式系统的概念中我们知道，各个主机之间通信和协调主要通过网络进行，所以，分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，
也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。

从进程角度看，两个程序分别运行在两个台主机的进程上，它们相互协作最终完成同一个服务（或者功能），那么理论上这两个程序所组成的系统，
也可以称作是“分布式系统”。

集中式：
集中式系统就是把所有的程序、功能都集中到一台主机上，从而往外提供服务的方式.
一个主机带多个终端，终端没有数据处理能力，仅负责数据的录入和输出。而运算、存储等全部在主机上进行。现在的银行系统，大部分都是这种集中式的系统。
集中式特点：
集中式系统的最大的特点就是部署结构非常简单，底层一般采用从IBM、HP等厂商购买到的昂贵的大型主机。因此无需考虑如何对服务进行多节点的部署，
也就不用考虑各节点之间的分布式协作问题。但是，由于采用单机部署。很可能带来系统大而复杂、难于维护、
发生单点故障（单个点发生故障的时候会波及到整个系统或者网络，从而导致整个系统或者网络的瘫痪）、扩展性差等问题。

集群：
当然这个两个程序可以是不同的程序，也可以是相同的程序。如果是相同的程序，我们又可以称之为“集群”。
所谓集群，就是将相同的程序，通过不断横向扩展以提高服务能力的方式。
集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。

具体组成：
1、节点：系统中按照协议完成计算工作的一个逻辑实体，可能是执行某些工作的进程或机器
2、网络：系统的数据传输通道，用来彼此通信。通信是具有方向性的。
3、存储：系统中持久化数据的数据库或者文件存储。

二、分布式系统有哪些特征:
和集中式系统相比，分布式系统的性价比更高、处理能力更强、可靠性更高、也有很好的扩展性。
1.分布性：分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。
2.透明性：系统资源被所有计算机共享。每台计算机的用户不仅可以使用本机的资源，还可以使用本分布式系统中其他计算机的资源(包括CPU、文件、打印机等)。
3.统一性：系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行。
4.通信性：系统中任意两台计算机都可以通过通信来交换信息。


三、设计分布式系统时，经常需要考虑如下的挑战：
1.异构性：分布式系统由于基于不同的网络、操作系统、计算机硬件和编程语言来构造，必须要考虑一种通用的网络通信协议来屏蔽异构系统之间的差异。一般交由中间件来处理这些差异。
2.缺乏全球时钟：在程序需要协作时，它们通过交换消息来协调它们的动作。紧密的协调经常依赖于对程序动作发生时间的共识，
 但是实际上网络上计算机同步时钟的准确性受到极大的限制，即没有一个正确时间的全局概念。这是通过网络发送消息作为唯一的通信方式这一事实带来的直接结果。
3.一致性：数据被分散或者复制到不同的机器上，如何保证各台主机之间的数据的一致性将成为一个难点。
4.故障的独立性：任何计算机都有可能故障，且各种故障不尽相同。他们之间出现故障的时机也是相互独立的。
 一般分布式系统要设计成被允许出现部分故障而不影响整个系统的正常使用。
5.并发：分布式系统的目的，是为了更好的共享资源。那么系统中的每个资源都必须被设计成在并发环境中是安全的。
6.透明性：分布式系统中任何组件的故障、或者主机的升级、迁移对于用户来说都是透明的，不可见的。
7.开放性：分布式系统由不同的程序员来编写不同的组件，组件最终要集成成为一个系统，那么组件所发布的接口必须遵守一定的规范且能够被互相理解。
8.安全性：加密用于给共享资源提供适当的保护，在网络上所有传递的敏感信息，都需要进行加密。拒绝服务攻击仍然是一个有待解决的问题。
9.可扩展性：系统要设计成随着业务量的增加，相应的系统也必须要能扩展来提供对应的服务。


四、分布式系统其他难点:
1. 网络因素
由于服务和数据分布在不同的机器上，每次交互都需要跨机器运行，这带来如下几个问题：
1.1. 网络延迟：性能、超时
同机房的网络IO还是比较块的，但是跨机房，尤其是跨IDC，网络IO就成为不可忽视的性能瓶颈了。并且延迟不是带宽，带宽可以随便增加，千兆网卡换成万兆只是成本的问题，
但延迟是物理限制基本不可能降低。

这带来的问题就是系统整体性能的降低，会带来一系列的问题比如资源的锁住，所以系统调用一般都要设置一个超时时间进行自我保护，
但是过度的延迟就会带来系统的RPC调用超时，引发一个令人头疼的问题：分布式系统调用的三态结果：成功、失败、超时。
不要小看这个第三态，这几乎是所有分布式系统复杂性的根源。

针对这个问题有一些相应的解决方案：异步化，失败重试。 而对于跨IDC数据分布带来的巨大网络因素影响，则一般会采用数据同步，代理专线等处理方式。

1.2. 网络故障：丢包、乱序、抖动。
这个可以通过将服务建立在可靠的传输协议上来解决，比如TCP协议。不过带来的是更多的网络交互。因此是性能和流量的一个trade off。
这个在移动互联网中更需要考虑。

2. 鱼与熊掌不可兼得——CAP定律
CAP理论是由Eric Brewer提出的分布式系统中最为重要的理论之一：
Consistency：[强]一致性，事务保障，ACID模型。
            (all nodes see the same data at the same time)
Availiablity：[高]可用性，冗余以避免单点，至少做到柔性可用（服务降级）。
             (a guarantee that every request receives a response about whether it was successful or failed)
Partition tolerance：[高]可扩展性（分区容忍性）：一般要求系统能够自动按需扩展，比如HBase。
              (the system continues to operate despite arbitrary message loss or failure of part of the system)
 

CAP原理告诉我们，这三个因素最多只能满足两个，不可能三者兼顾。对于分布式系统来说，分区容错是基本要求，所以必然要放弃一致性。对于大型网站来说，
分区容错和可用性的要求更高，所以一般都会选择适当放弃一致性。对应CAP理论，NoSQL追求的是AP，而传统数据库追求的是CA，
这也可以解释为什么传统数据库的扩展能力有限的原因。

在CAP三者中，“可扩展性”是分布式系统的特有性质。分布式系统的设计初衷就是利用集群多机的能力处理单机无法解决的问题。当需要扩展系统性能时，
一种做法是优化系统的性能或者升级硬件(scale up)，一种做法就是“简单”的增加机器来扩展系统的规模(scale out)。好的分布式系统总在追求”线性扩展性”，
即性能可以随集群数量增长而线性增长。

可用性和可扩展性一般是相关联的，可扩展行好的系统，其可用性一般会比较高，因为有多个服务(数据)节点，不是整体的单点。所以分布式系统的所有问题，
基本都是在一致性与可用性和可扩展性这两者之间的一个协调和平衡。对于没有状态的系统，不存在一致性问题，根据CAP原理，它们的可用性和分区容忍性都是很高，
简单的添加机器就可以实现线性扩展。而对于有状态的系统，则需要根据业务需求和特性在CAP三者中牺牲其中的一者。一般来说，
交易系统类的业务对一致性的要求比较高，一般会采用ACID模型来保证数据的强一致性，所以其可用性和扩展性就比较差。
而其他大多数业务系统一般不需要保证强一致性，只要最终一致就可以了，它们一般采用BASE模型，用最终一致性的思想来设计分布式系统，
从而使得系统可以达到很高的可用性和扩展性。

CAP定律其实也是衡量分布式系统的重要指标，另一个重要的指标是性能。
一致性模型
主要有三种：
Strong Consistency（强一致性）：新的数据一旦写入，在任意副本任意时刻都能读到新值。比如：文件系统，RDBMS，Azure Table都是强一致性的。
                    强一致性描述了所有节点的数据高度一致，无论从哪个节点读取，都是一样的。无需担心同一时刻会获得不同的数据。是级别最高的实现的代价比较高
Week Consistency（弱一致性）：不同副本上的值有新有旧，需要应用方做更多的工作获取最新值。比如Dynamo。
Evantual Consistency（最终一致性）：一旦更新成功，各副本的数据最终将达到一致。
从这三种一致型的模型上来说，我们可以看到，Weak和Eventually一般来说是异步冗余的，而Strong一般来说是同步冗余的(多写)，异步的通常意味着更好的性能，
但也意味着更复杂的状态控制。同步意味着简单，但也意味着性能下降。
其实弱一致性就是在系统上通过业务可接受的方式换取了一些系统的低复杂度和可用性

以及其他变体：
Causal Consistency（因果一致性）：如果Process A通知Process B它已经更新了数据，那么Process B的后续读取操作则读取A写入的最新值，
而与A没有因果关系的C则可以最终一致性。
Read-your-writes Consistency（读你所写一致性）：如果Process A写入了最新的值，那么 Process A的后续操作都会读取到最新值。
但是其它用户可能要过一会才可以看到。
Session Consistency（会话一致性）：一次会话内一旦读到某个值，不会读到更旧的值。
Monotonic Read Consistency（单调一致性）：一个用户一旦读到某个值，不会读到比这个值更旧的值，其他用户不一定。
等等。

其中最重要的变体是第二条：Read-your-Writes Consistency。特别适用于数据的更新同步，用户的修改马上对自己可见，但是其他用户可以看到他老的版本。
Facebook的数据同步就是采用这种原则。


五、如何来设计分布式:
设计分布式系统的本质就是“如何合理将一个系统拆分成多个子系统部署到不同机器上”。
所以首要考虑的问题是如何合理的将系统进行拆分。由于拆分后的各个子系统不可能孤立的存在，必然是通过网络进行连接交互，所以它们之间如何通信变得尤为重要。
当然在通信过程要识别“敌我”，防止信息在传递过程中被拦截和窜改，这就涉及到安全问题了。分布式系统要适应不断增长的业务需求，那么就需要考虑其扩展性。
分布式系统还必须要保证可靠性和数据的一致性。

概况起来，在设计分布式系统时，应考虑以下几个问题：
1.系统如何拆分为子系统？
2.如何规划子系统间的通信？
3.通信过程中的安全如何考虑？
4.如何让子系统可以扩展？
5.子系统的可靠性如何保证？
6.数据的一致性是如何实现的？

实际上，上面的每一个问题都不是简单的问题。还好，我们要感谢开源，让这个时代的技术可以共享，让实现复杂系统的成本越来越低。
比如：
我们在设计通信时，我们可以采用面向消息的中间件：
例如Apache ActiveMQ、RabbitMQ、Apache RocketMQ、Apache Kafka等，也有类似与 Google Protocol Buffer、Thrift等 RPC框架。
在设计分布式计算时，我们分布式计算可以采用 MapReduce、Apache Hadoop、Apache Spark 等。
在大数据和分布式存储方面，我们可以选择 Apache Hbase、Apache Cassandra、Memcached、Redis、MongoDB等。
在分布式监控方面，常用的技术包括Nagios、Zabbix、Consul、ZooKeeper等。


六、常用的分布式方案
分布式应用和服务：
将应用和服务进行分层和分割，然后将应用和服务模块进行分布式部署。这样做不仅可以提高并发访问能力、减少数据库连接和资源消耗，
还能使不同应用复用共同的服务，使业务易于扩展。

分布式静态资源：
对网站的静态资源如JS、CSS、图片等资源进行分布式部署可以减轻应用服务器的负载压力，提高访问速度。

分布式数据和存储：
大型网站常常需要处理海量数据，单台计算机往往无法提供足够的内存空间，可以对这些数据进行分布式存储。

分布式计算：
随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。分布式计算将该应用分解成许多小的部分，
分配给多台计算机进行处理。这样可以节约整体计算时间，大大提高计算效率。

七、分布式系统设计策略
1、重试机制
 一般情况下，写一段网络交互的代码，发起rpc或者http，都会遇到请求超时而失败情况。可能是网络抖动(暂时的网络变更导致包不可达，比如拓扑变更)或者对端挂掉。
 这时一般处理逻辑是将请求包在一个重试循环块里。
 eg:
int retry = 3;  
while(!request() && retry--)  
    sched_yield();   // or usleep(100) 
此种模式可以防止网络暂时的抖动，一般停顿时间很短，并重试多次后，请求成功！但不能防止对端长时间不能连接(网络问题或进程问题)。

2、心跳机制
心跳顾名思义，就是以固定的频率向其他节点汇报当前节点状态的方式。收到心跳，一般可以认为一个节点和现在的网络拓扑是良好的。当然，心跳汇报时，
一般也会携带一些附加的状态、元数据信息，以便管理。
但心跳不是万能的，收到心跳可以确认ok，但是收不到心跳却不能确认节点不存在或者挂掉了，因为可能是网络原因倒是链路不通但是节点依旧在工作。
所以切记，”心跳“只能告诉你正常的状态是ok，它不能发现节点是否真的死亡，有可能还在继续服务。(后面会介绍一种可靠的方式 -- Lease机制)

3、副本
副本指的是针对一份数据的多份冗余拷贝，在不同的节点上持久化同一份数据，当某一个节点的数据丢失时，可以从副本上获取数据。
数据副本是分布式系统解决数据丢失异常的仅有的唯一途径。当然对多份副本的写入会带来一致性和可用性的问题，比如规定副本数为3，同步写3份，
会带来3次IO的性能问题。还是同步写1份，然后异步写2份，会带来一致性问题，比如后面2份未写成功其他模块就去读了

4、中心化/无中心化
系统模型这方面，无非就是两种：
中心节点，例如mysql的MSS单主双从、MongDB Master、HDFS NameNode、MapReduce JobTracker等，有1个或几个节点充当整个系统的核心元数据及节点管理工作，
其他节点都和中心节点交互。这种方式的好处显而易见，数据和管理高度统一集中在一个地方，容易聚合，就像领导者一样，其他人都服从就好。简单可行。
但是缺点是模块高度集中，容易形成性能瓶颈，并且如果出现异常，就像群龙无首一样。
无中心化的设计，例如cassandra、zookeeper，系统中不存在一个领导者，节点彼此通信并且彼此合作完成任务。好处在于如果出现异常，不会影响整体系统，
局部不可用。缺点是比较协议复杂，而且需要各个节点间同步信息。


分布式系统基础重要要点：

1.对外提供无状态节点，内部实现具体有状态或者无状态节点逻辑，节点即可以是提供服务，也可以是存储数据。
2.拜占庭问题，在分布式系统中的使用，目的是保证服务可用，而不是找出错误的节点，如果。
3.异常常见情况，机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP。可能是收到消息后宕机、也可能是处理完成以后机器宕机、
 处理完成任务后发送确认消息是网络异常。也有可能是发出去的消息丢失，或者发送确认消息时丢失。可能先发送出去的数据后收到
4.分布式状态、成功、失败、超时。超时的情况，不能判断是否成功，原有同上。
5.数据存储在机械硬盘上，随时有可能发生异常，导致数据没有能正确存储。
6.无法归类的异常，比如，系统的处理能力时高、时低，的诡异行为。
7.即使是小概率事件，在每天百万、千万、及以上的运算量时也会上升为大概率事件。
8.副本提高数据的冗余，提高系统的可用性，但是在使用副本代来好处的同时，也导致维护副本需要成本。如副本的一致性，多个副本一致性，多个副本直接可能到不一致。
9.一致性级别：强一致性、单调一致性，读取最新数据、会话一致性，通过版本读取统一值。最终一致性、弱一致性。
 
指标：
分布式系统性能指标：吞吐量、响应延迟、并发量；常用单位QPS，即每秒钟的处理能力。高吞吐量会带来低响应、他们之间是相互制约关系。
可用性指标：可以服务时间和非服务时间的比率和请求的成功和失败次数来衡量。
可扩展性指标：实现能水平扩展，增加低配置的机器即可以实现更大的运算量，和更高的处理能力。
一致性指标：实现副本间的一致性能力，一致性需要严格考量是否业务允许。


分布式原理：
1．哈希方式，把不同的值进行哈希运算，映射到，不同的机器或者节点。考虑冗余的时候可以把多个哈希值映射到同一个地方。哈希的实现方式，取余。
其实现扩展时，比较困难，数据分散在很多机器上，扩展的时候要从个机器上获取数据。而且容易出现分布不均有的情况。常见的哈希，用IP、URL、ID、或者固定的值进行哈希，总是得到相同的结果。

2．按数据范围分布，比如ID在1~20的在机器A上，ID在21~40的在机器B上，ID在40~60的在机器C上实现，ID在60~100的分布在机器D上，数据分布比较均匀。
  如果某个节点处理能力有限，可以直接分裂该节点。维护数据分布的元信息，可能出现单点瓶颈。几千机器，每个机器又划分为N个范围，
  导致需要维护的数据分布范围元数据过大，导致可能需要几台机器实现。一定要严格控制元数据量，进可能的减少元数据的存储。

3．按数据量分布，另一类常用的数据分布方式则是按照数据量分布数据。与哈希方式和按数据范围方式不同，数据量分布数据与具体的数据特征无关，
  而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块（chunk），不同的数据块分布到不同的服务器上。
  与按数据范围分布数据的方式类似的是，按数据量分布数据也需要记录数据块的具体分布情况，并将该分布信息作为元数据使用元数据服务器管理。
  由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。当集群需要重新负载均衡时，
  只需通过迁移数据块即可完成。集群扩容也没有太大的限制，只需将部分数据库迁移到新加入的机器上即可以完成扩容。
  按数据量划分数据的缺点是需要管理较为复杂的元信息，与按范围分布数据的方式类似，当集群规模较大时，元信息的数据量也变得很大，
  高效的管理元信息成为新的课题。

4．一致性哈希，构造哈希环，有哈希域[0,10]，则构造3个部分，[1,4)/[4,9)/[9,10),[0,1)/分成了3个部分，这3部分是一个环状，增加机器时，
  变动的是其附近的节点，分担的是附近节点的压力，其元数据的维护和按数据量分布一样。其未来的扩展，可以实现多个需节点。

5．构建映射元数据，建立映射表的方式。

6．副本与数据分布，把一个数据副本分散到多台服务器上。比如应用A的数据，存储在A、B、C ，3台机器上，如果3台机器中，其中一台出现问题，
  请求被处理到其他2台机器上，如果加机器恢复，还需要从另外2台机器上，Copy数据，又增加了这2台机器的负担。如果我们有应用A和应用B，
  各自有3台机器，那么我们可以把A应用分散在6台机器上，B应用也分散在6台机器上，可以实现相同的数据备份，但是应用存储的数据被分散了。
  某台机器损害，只是把该机器所承担的负载平均分配到了，另外5台机器上。恢复数据从5台机器恢复，其速度快和给各台服务器的压力都不大，
  而且可以实现机器损害，更换完全不影响应用。其原理是多个机器互为副本，是比较理想的实现负载分压的方式。

7．分布式计算思想，移动数据不如移动计算，就进计算原则，减少跨进程、跨网络、等跨度较大的实现，把计算所需的资源尽可能的靠近。因为可能出现网络、远程机器的瓶颈。

8．常见分布式系统数据分布方式: GFS、HDFS:按数据量分布；Map reduce 按GFS的数据分布做本地化；BigTable、HBase按数据范围分布；
   Pnuts按哈希方式或者数据范围分布，可以选择；Dynamo、Cassndra按一致性哈希；Mola、Armor、BigPipe按哈希方式分布；Doris按哈希方式和按数据量分布组合。




数据副本协议
1．副本一定要满足一定的可用性和一致性要求、具体容错能力，即使出现一些问题也能提供可靠服务。
2．数据副本的基本协议，中心化和去中心化2种基本的副本控制协议。
3．中心化副本控制协议的基本思路是由一个中心节点协调副本数据的更新、维护副本之间的一致性。中心化副本控制协议的优点是协议相对较为简单，
  所有的副本相关的控制交由中心节点完成。并发控制将由中心节点完成，从而使得一个分布式并发控制问题，简化为一个单机并发控制问题。
  控制问题，简化为一个单机并发控制问题。所谓并发控制，即多个节点同时需要修改副本数据时，需要解决“写写”、“读写”等并发冲突。
  单机系统上常用加锁等方式进行并发控制。对于分布式并发控制，加锁也是一个常用的方法，但如果没有中心节点统一进行锁管理，就需要完全分布式化的锁系统，
  会使得协议非常复杂。中心化副本控制协议的缺点是系统的可用性依赖于中心化节点，当中心节点异常或与中心节点通信中断时，
  系统将失去某些服务（通常至少失去更新服务），所以中心化副本控制协议的缺点正是存在一定的停服务时间。即存在单点问题，即使中心化节点是一个集群，
  也只不过是一个大的单点。
4．副本数据同步常见问题，1）网络异常，导致副本没有得到数据；2）数据脏读，主节点数据已经更新，但是由于某种原因，没有得到最新数据；
   3）增加新节点没有得到主节点数据，而读数据时从新节点读数据导致，没有得到数据。
5．去中心化副本控制协议没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。
  从而去中心化协议没有因为中心化节点异常而带来的停服务等问题。然而，没有什么事情是完美的，去中心化协议的最大的缺点是协议过程通常比较复杂。
  尤其当去中心化协议需要实现强一致性时，协议流程变得复杂且不容易理解。由于流程的复杂，去中心化协议的效率和性能较低。
6．Paxos是唯一在工程中得到应用的强一致性去中心化副本控制协议。ZooKeeper、Chubby，就是该协议的应用。
  Zookeeper用Paxos协议选择Leader，用Lease协议控制数据是否有效。用Quorum协议把Leader的数据同步到follow。
  Zeekeeper，实现Quorum写入时，如果没有完全写入成功，则所有的follow机器，反向向Leader写数据，写入数据后follow又向Leader同步数据，
  保持一致，如果是失败的数据先写入，你们follow同步到原来的数据，相对于回滚；如是是最新的数据先写入Leader则就是完成了最新数据的更新。
7．Megastore，使用的是改进型行Paxos协议。
8．Dynamo / Cassandra使用基于一致性哈希的去中心化协议。Dynamo使用Quorum机制来管理副本。
9．Lease机制是最重要的分布式协议，广泛应用于各种实际的分布式系统中。
  1）Lease通常定义为：颁发者在一定期限内給予持有者一定权利的协议。
  2）Lease 表达了颁发者在一定期限内的承诺，只要未过期颁发者必须严格遵守 lease 约定的承诺；
  3）Lease 的持有者在期限内使用颁发者的承诺，但 lease 一旦过期必须放弃使用或者重新和颁发者续约。
  4）的影响。中心服务器发出的lease的含义为：在lease的有效期内，中心服务器保证不会修改对应数据的值。
  5）可以通过版本号、过多上时间、或者到某个固定时间点认为Lease证书失效。其原理和我们的Cache一样，比如浏览器缓存道理一致。其要求时间时钟同步，
    因为数据完全依赖于期限。
10．心跳（heartbeat）检测不可靠,假如检测及其Q，被检测机器A，可能由于Q发起检测，但是A的回应被阻塞，导致Q认为A宕机，阻塞很快恢复，
  导致根据心跳检测来做判断不可靠；也可能是他们之间的网络断开；也可能是Q机器本身异常导致认为A机器宕机；如果根据Q的检测结果，
  来判断很可能出现多个主机的情况。
11．Write-all-read-one（简称WARO）是一种最简单的副本控制规则，顾名思义即在更新时写所有的副本，只有在所有的副本上更新成功，才认为更新成功，
  从而保证所有的副本一致，这样在读取数据时可以读任一副本上的数据。写多份，读从其中一份读取。
12．quorum协议，其实就是读取成功的副本数大于失败的副本数，你们读取的副本里面一定包含了最新的副本。
13．Mola*和Armor*系统中所有的副本管理都是基于Quorum，即数据在多数副本上更新成功则认为成功。
14．Big Pipe*中的副本管理也是采用了WARO机制。
 

日志技术
1．日志技术是宕机恢复的主要技术之一。日志技术最初使用在数据库系统中。严格来说日志技术不是一种分布式系统的技术，但在分布式系统的实践中，
  却广泛使用了日志技术做宕机恢复，甚至如BigTable等系统将日志保存到一个分布式系统中进一步增强了系统容错能力。
2．两种比较实用的日志技术Redo Log与No Redo/No undo Log。
3．数据库的日志主要分为Undo Log、Redo Log、Redo/Undo Log与No Redo/No Undo Log。这四类日志的区别在更新日志文件和数据文件的时间点要求不同，
  从而造成性能和效率也不相同。
4．本节介绍另一种特殊的日志技术“No Undo/No Redo log”，这种技术也称之为“0/1目录”(0/1 directory)。还有一个主记录，记录当前工作目录，
  比如老数据在0目录下，新数据在1目录下，我们访问数据时，通过主纪录，记录当前是工作在那个目录下，如果是工作在目录0下，取目录0数据，反之取1目录数据。
5．MySQL的主从库设计也是基于日志。从库只需通过回放主库的日志，就可以实现与主库的同步。由于从库同步的速度与主库更新的速度没有强约束，
  这种方式只能实现最终一致性。
6．在单机上，事务靠日志技术或MVCC等技术实现。
7．两阶段提交的思路比较简单，在第一阶段，协调者询问所有的参与者是否可以提交事务（请参与者投票），所有参与者向协调者投票。在第二阶段，
  协调者根据所有参与者的投票结果做出是否事务可以全局提交的决定，并通知所有的参与者执行该决定。在一个两阶段提交流程中，参与者不能改变自己的投票结果。
  两阶段提交协议的可以全局提交的前提是所有的参与者都同意提交事务，只要有一个参与者投票选择放弃(abort)事务，则事务必须被放弃。可以这么认为，
  两阶段提交协议对于这种超时的相关异常也没有很好的容错机制，整个流程只能阻塞在这里，且对于参与者而言流程状态处于未知，参与者即不能提交本地节点上的事务，
  也不能放弃本地节点事务。
8．第一、两阶段提交协议的容错能力较差。
9．第二、两阶段提交协议的性能较差。一次成功的两阶段提交协议流程中，协调者与每个参与者之间至少需要两轮交互4个消息“prepare”、“vote-commit”、“global-commit”、
  “确认global-commit”。过多的交互次数会降低性能。另一方面，协调者需要等待所有的参与者的投票结果，一旦存在较慢的参与者，会影响全局流程执行速度。
10．顾名思义，MVCC即多个不同版本的数据实现并发控制的技术，其基本思想是为每次事务生成一个新版本的数据，
  在读数据时选择不同版本的数据即可以实现对事务结果的完整性读取。在使用MVCC时，每个事务都是基于一个已生效的基础版本进行更新，事务可以并行进行。
  其思想是根据版本号，在多个节点取同一个版本号的数据。

11．MVCC的流程过程非常类似于SVN等版本控制系统的流程，或者说SVN等版本控制系统就是使用的MVCC思想。


其他分支

分布式存储概念
大规模分布式存储系统的定义如下：“分布式存储系统是大量普通PC服务器通过Internet互联，对外作为一个整体提供存储服务。”

分布式存储系统具有如下几个特性：
1.可扩展：分布式存储系统可以扩展到几百台甚至几千台的集群规模，而且，随着集群规模的增长，系统整体性能表现为线性增长。
2.低成本：分布式存储系统的自动容错、自动负载均衡机制使其可以构建在普通PC机之上。另外，线性扩展能力也使得增加、减少机器非常方便，可以实现自动运维。
3.高性能：无论是针对整个集群还是单台服务器，都要求分布式存储系统具备高性能。
4.易用：分布式存储系统需要能够提供易用的对外接口，另外，也要求具备完善的监控、运维工具，并能够方便地与其他系统集成，例如，从Hadoop云计算系统导入数据。

分布式存储系统的挑战主要在于数据、状态信息的持久化，要求在自动迁移、自动容错、并发读写的过程中保证数据的一致性。
分布式存储涉及的技术主要来自两个领域：分布式系统以及数据库，如下所示：
1.数据分布：如何将数据分布到多台服务器才能够保证数据分布均匀？数据分布到多台服务器后如何实现跨服务器读写操作？
2.一致性：如何将数据的多个副本复制到多台服务器，即使在异常情况下，也能够保证不同副本之间的数据一致性？
3.容错：如何检测到服务器故障？如何自动将出现故障的服务器上的数据和服务迁移到集群中其他服务器？
4.负载均衡：新增服务器和集群正常运行过程中如何实现自动负载均衡？数据迁移的过程中如何保证不影响已有服务？
5.事务与并发控制：如何实现分布式事务？如何实现多版本并发控制？
6.易用性：如何设计对外接口使得系统容易使用？如何设计监控系统并将系统的内部状态以方便的形式暴露给运维人员？
7.压缩/解压缩：如何根据数据的特点设计合理的压缩/解压缩算法？如何平衡压缩算法节省的存储空间和消耗的CPU计算资源？

分布式存储系统挑战大，研发周期长，涉及的知识面广。一般来讲，工程师如果能够深入理解分布式存储系统，理解其他互联网后台架构不会再有任何困难。



下面简要概括一下分布式系统涉及的技术：
1、远程服务框架：远程服务框架是分布式系统中不可或缺的，对于远程服务框架来说，
  关键的几个点是：服务消费者、服务提供者、协议（java序列号、反序列化、hessian协议等等）、网络（NIO、多路复用等等）、地址列表。
  其中协议的选择、网络的优化以及地址列表的获取是难点。目前业界比较有名的开源框架有：Apache Thrift（跨语言）、Dubbo等等。
2、消息中间件：引入消息中间件是用来降低应用之间的耦合的。对于上游应用来说，它往往会依赖非常多的下游应用，
  如果全部用同步调用（远程过程调用就是同步调用）的方式，那么上游应用必须等到所有下游应用全部执行成功后，才能返回成功，这种等待往往是不可忍受的。
  为此，需要将同步异步化，而消息中间件可以实现这种异步化。使用消息中间件后，上游应用只需要向消息服务器发送消息就可以了，它不用等待所有应用执行完，
  它可以提前返回执行成功，后续，消息服务器会去向这些下游应用发消息，通知它们执行。虽然异步化降低了应用之间的耦合，但也引入了新的问题。
  比如，下游系统执行的时候挂了，而这时候上游系统其实是返回的成功，这样就导致了状态不一致。要解决这个问题，就需要使用消息中间件本身的重试机制，
  对于一些系统异常，需要重试直到成功，对于应用本身，又需要对这种重试进行幂等处理，并且使用正向状态机管理整个系统的状态。
  消息中间件的规范可以参考JMS，业界也有成熟的开源方案，比如ActivityMQ。
3、wrapper层：warpper层主要是上面两个入口的包装层，进行一些异常管理，日志管理和AOP拦截
4、MVC框架：这个不用多说了吧，个人比较喜欢spring MVC，有空可以阅读下源代码
5、分布式缓存：。
6、分布式日志：。
7、文件存储系统：
8、IOC容器：目前用的最多的是spring容器，对于spring的核心的几个模块，可以参考《spring源码深度解析》
9、ORM框架：ORM最好使用ibatis，不推荐hibernate，因为ibatis可以控制sql，在大数据量读写下，必须对sql进行优化。
  至于ibatis本身的架构，有空我会单独写文章。
10、调度引擎：用于执行异步的定时任务的引擎，待填坑。。
11、分布式数据访问层：正在学习中。。
12、配置中心：统一的配置管理中心，待填坑。。
13、web容器：tomcat，结合《tomcat源代码分析》和tomcat本身的源代码，应该可以很容易的懂tomcat的架构和实现。
14、业务逻辑层：流程引擎和规则引擎，配合spring来治理极易变动的业务逻辑，后续有空单独写个流程引擎


问题举例：
不同操纵系统之间的特点；
网络端口管理与分发；
哪些网络协议可以帮助我们更好的完成工作，监控虚拟机的时候是在虚机上加代div理好还是用协议去控制；
硬件是否支持分布式，在扩展过程中对于.net C#的兼容怎么样；
什么时候使用多线程，在把线程交给程序调度的时候我们怎么控制和捕捉线程的异常；
日志系统对于整个分散的系统是多么的重要；
何时使用关系数据库，什么时候使用Nosql；
消息队列用擅长的MSMQ还是RabbitMQ.
怎样有效的和其他部门的同事沟通；
用什么样的方式去有效调度不同语言开发的系统；
测试用例对于大系统从零散到完整是多么的重要；
系统标准，代码原则对于后期的维护余扩展是多么的重要；

